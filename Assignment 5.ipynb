{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine\n",
    "\n",
    "The restricted Boltzman Machine model is the Joint Probability Distribution which is specified by the Energy Function :\n",
    "\n",
    "\\begin{equation}\n",
    "P(v,h) = \\frac{1}{Z} e^{-E(v,h)}\n",
    "\\end{equation}\n",
    "\n",
    "The energy function for the RBM is stated as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "E(v,h) = -b^{T} v - c^{T} h - v^{T} W h \n",
    "\\end{equation}\n",
    "\n",
    "We also have the Partition Function Z which is the normalizing constant.\n",
    "\n",
    "\\begin{equation}\n",
    "Z = \\Sigma_{v} \\Sigma_{h} e^{-E(v,h)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "In Boltzmann Machines, the partition function Z is intractable and hence implies that the normalized Joint Probability Distribution _P(v)_ is also intractable to evaluate. Even though this is the case, the bipartitie graph structure of the RBM has a special property that the visible and hidden units are conditionally independent, given one another. \n",
    "\n",
    "\\begin{equation}\n",
    "P(h|v) = \\frac{P(h,v)}{P(v)}\n",
    "       = \\frac{1}{P(v)} \\frac{1}{Z} exp\\{b^{T} v + c^{T} h + v^{T} W h\\}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    = \\frac{1}{Z'} exp\\{\\Sigma_j c_j h_j + \\Sigma_j v^T W_j h_j \\}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    = \\frac{1}{Z'} \\Pi_j exp \\{ c_j h_j + v^T W_j h_j\\}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(h_j = 1,v) = \\frac{\\hat{P}(h_j = 1,v)}{\\hat{P}(h_j = 0,v) + \\hat{P}(h_j = 1,v)}\n",
    "             = \\frac{exp\\{c_j + v^T W_j \\}}{exp\\{0\\} + exp\\{c_j + v^T W_j \\}}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore:\n",
    "\\begin{equation}\n",
    "P(h_j = 1|v) = \\sigma(c_j + v^T W_j )\n",
    "\\end{equation}\n",
    "\n",
    "Similary from Eq 8 we can say that:\n",
    "\\begin{equation}\n",
    "P(v|h) = \\frac{1}{Z'} \\Pi_k exp \\{b_k + h^T W_k \\}\n",
    "\\end{equation}\n",
    "Therefore:\n",
    "\\begin{equation}\n",
    "P(v_k = 1|h) = \\sigma(b_k + h^T W_k)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "if not os.path.exists('outRBM/'):\n",
    "    os.makedirs('outRBM/')\n",
    "\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "\n",
    "mb_size = 16\n",
    "h_dim = 20\n",
    "\n",
    "W = np.random.randn(X_dim, h_dim) * 0.001\n",
    "a = np.random.randn(h_dim) * 0.001\n",
    "b = np.random.randn(X_dim) * 0.001\n",
    "\n",
    "\n",
    "def sigm(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def infer(X):\n",
    "    # mb_size x x_dim -> mb_size x h_dim\n",
    "    return sigm(X @ W)\n",
    "\n",
    "\n",
    "def generate(H):\n",
    "    # mb_size x h_dim -> mb_size x x_dim\n",
    "    return sigm(H @ W.T)\n",
    "\n",
    "\n",
    "# Here we find the Contrastive Divergence\n",
    "# ----------------------\n",
    "# Approximate the log partition gradient Gibbs sampling\n",
    "\n",
    "alpha = 0.1\n",
    "K = 15  # Num. of Gibbs sampling step\n",
    "\n",
    "for t in range(1, 1001):\n",
    "    X_mb = (mnist.train.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
    "    g = 0\n",
    "    g_a = 0\n",
    "    g_b = 0\n",
    "\n",
    "    for v in X_mb:\n",
    "        # E[h|v,W]\n",
    "        h = infer(v)\n",
    "\n",
    "        # Gibbs sampling steps\n",
    "        # --------------------\n",
    "        v_prime = np.copy(v)\n",
    "\n",
    "        for k in range(K):\n",
    "            # h ~ p(h|v,W)\n",
    "            h_prime = np.random.binomial(n=1, p=infer(v_prime))\n",
    "            # v ~ p(v|h,W)\n",
    "            v_prime = np.random.binomial(n=1, p=generate(h_prime))\n",
    "\n",
    "        # E[h|v',W]\n",
    "        h_prime = infer(v_prime)\n",
    "\n",
    "        # Compute data gradient\n",
    "        grad_w = np.outer(v, h) - np.outer(v_prime, h_prime)\n",
    "        grad_a = h - h_prime\n",
    "        grad_b = v - v_prime\n",
    "\n",
    "        # Accumulate minibatch gradient\n",
    "        g += grad_w\n",
    "        g_a += grad_a\n",
    "        g_b += grad_b\n",
    "\n",
    "    # Monte carlo gradient\n",
    "    g *= 1 / mb_size\n",
    "    g_a *= 1 / mb_size\n",
    "    g_b *= 1 / mb_size\n",
    "\n",
    "    # Update to maximize\n",
    "    W += alpha * g\n",
    "    a += alpha * g_a\n",
    "    b += alpha * g_b\n",
    "\n",
    "\n",
    "# Visualization\n",
    "# -------------\n",
    "\n",
    "def plot(samples, size, name):\n",
    "    size = int(size)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(size, size), cmap='Greys_r')\n",
    "\n",
    "    plt.savefig('outRBM/{}.png'.format(name), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "X = (mnist.test.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
    "\n",
    "H = np.random.binomial(n=1, p=infer(X))\n",
    "plot(H, np.sqrt(h_dim), 'H')\n",
    "\n",
    "X_recon = (generate(H) > 0.5).astype(np.float)\n",
    "plot(X_recon, np.sqrt(X_dim), 'V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Variational AutoEncoder we convert the input into a latent space and calculate the mean and standard deviation. By combining this, we get a new distribution of latent space which provides a better output than a regular autoencoder. \n",
    "\n",
    "We shall consider the following definitions when deriving the VAE.\n",
    "\n",
    "    1. X      - The data to be modeled\n",
    "    2. z      - Latent Variable\n",
    "    3. P(X)   - Probability distribution of Data\n",
    "    4. P(z)   - Probability Distribution of Latent Variable\n",
    "    P(X|z)    - Probability Distribution of the generated Data given the Latent Variable.\n",
    "\n",
    "In VAE, we try to find the latent space _z_ using the data _X_. Hence we try to infer _P(z)_ using _P(z|X)_. But we do not know _P(z|X)_, hence we use Variational Inference and approach it as an optimization problem. We do this by actual distribution _P(z|X)_ using a simpler distribution such as Gaussian and then find the difference between the two distributions using KL Divergence.\n",
    "\n",
    "For inferring _P(z|X)_ using _Q(z|X)_, we have the KL Divergence as :\n",
    "\\begin{equation}\n",
    "D_{KL}[Q(z|X)∥P(z|X)] = \\Sigma_z Q(z|X) log \\frac{Q(z|X)}{P(z|X)}\n",
    "                      = E[log Q(z|X) - log P(z|X)]\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bayes' Rule, we can expand _P(z|X)_ as the following:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}[Q(z|X)∥P(z|X)] = E[log Q(z|X) - log \\frac{P(X|z) P(z)}{P(X)}]\n",
    "= E[log Q(z|X) − log P(X|z) − log P(z) + log P(X)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expection we are finding is over _z_, hence independent of _x_. Therefore, we we move _x_ out of the expectation. \n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}[Q(z|X)∥P(z|X)] = E[log Q(z|X) − log P(X|z) − log P(z) ] + log P(X)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "D_{KL}[Q(z|X)∥P(z|X)] - log P(X)  = E[log Q(z|X) − log P(X|z) − log P(z)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation can then be written as another KL Divergence. That is shown below. \n",
    "\n",
    "\\begin{equation}\n",
    "log P(X) - D_{KL}[Q(z|X)∥P(z|X)] = E[log P(X|z) - (log Q(z|X) − log P(z))]\n",
    "= E[log P(X|z)] - E[log Q(z|X) - log P(z)]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This is the final objective functions that we arrive from the derivation:\n",
    "\n",
    "\\begin{equation}\n",
    "log P(X) - D_{KL}[Q(z|X)∥P(z|X)] = E[log P(X|z)] - D_{KL}[Q(z|X)∥P(z)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla VAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T02:10:43.834732Z",
     "start_time": "2019-05-10T02:10:36.087873Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-7e44334aa31a>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/akash/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/akash/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/akash/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/akash/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/akash/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/akash/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/akash/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\n",
    "mb_size = 64\n",
    "z_dim = \n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 256\n",
    "c = 0\n",
    "lr = 1e-3\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "# =============================== Q(z|X) ======================================\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "Q_W2_mu = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_mu = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "Q_W2_sigma = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_sigma = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "\n",
    "def Q(X):\n",
    "    h = tf.nn.relu(tf.matmul(X, Q_W1) + Q_b1)\n",
    "    z_mu = tf.matmul(h, Q_W2_mu) + Q_b2_mu\n",
    "    z_logvar = tf.matmul(h, Q_W2_sigma) + Q_b2_sigma\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random_normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var / 2) * eps\n",
    "\n",
    "\n",
    "# =============================== P(X|z) ======================================\n",
    "\n",
    "P_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
    "P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "\n",
    "def P(z):\n",
    "    h = tf.nn.relu(tf.matmul(z, P_W1) + P_b1)\n",
    "    logits = tf.matmul(h, P_W2) + P_b2\n",
    "    prob = tf.nn.sigmoid(logits)\n",
    "    return prob, logits\n",
    "\n",
    "\n",
    "# =============================== TRAINING ====================================\n",
    "\n",
    "z_mu, z_logvar = Q(X)\n",
    "z_sample = sample_z(z_mu, z_logvar)\n",
    "_, logits = P(z_sample)\n",
    "\n",
    "# Sampling from random z\n",
    "X_samples, _ = P(z)\n",
    "\n",
    "# E[log P(X|z)]\n",
    "recon_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X), 1)\n",
    "# D_KL(Q(z|X) || P(z)); calculate in closed form as both dist. are Gaussian\n",
    "kl_loss = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1)\n",
    "# VAE loss\n",
    "vae_loss = tf.reduce_mean(recon_loss + kl_loss)\n",
    "\n",
    "solver = tf.train.AdamOptimizer().minimize(vae_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-10T02:10:50.205Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(1000000):\n",
    "    X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "\n",
    "    _, loss = sess.run([solver, vae_loss], feed_dict={X: X_mb})\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {:.4}'. format(loss))\n",
    "        print()\n",
    "\n",
    "        samples = sess.run(X_samples, feed_dict={z: np.random.randn(16, z_dim)})\n",
    "        print (type(samples))\n",
    "        #fig = plot(samples)\n",
    "        #samples('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        #plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References : \n",
    "    1.https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/\n",
    "    2.https://github.com/wiseodd/generative-models/blob/master/RBM/rbm_binary_cd.py\n",
    "    3.https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
    "    4.http://anotherdatum.com/vae2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
